{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb9EIYAir7vM"
   },
   "source": [
    "# KoSOLAR-v0.2-gugutypus-10.7B\n",
    "<font color='orange'>LoRA를 활용한 **KoSOLAR-10.7B-v0.2** fine-tuning with **KOR-gugugu-platypus-set**</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?id=1G_OreIMXjxZNwo9rpJDFHMIslRB1RwOM\" height=512, width=512>\n",
    "\n",
    "**gugutypus** = gugugo + platypus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqCodfTcS_RC"
   },
   "source": [
    "-------\n",
    "## Notebook Summary\n",
    "\n",
    "\n",
    "### 1. KoSOLAR-v0.2-gugutypus-10.7B 📖\n",
    "\n",
    "- Code 환경설정\n",
    "- Hyperparameters in LLM\n",
    "- <font color='#e79531'><b>LoRA 🔥</b></font>\n",
    "- Instruction input-output dataset\n",
    "- <font color='#9e95f2'><b>LLM fine-tuning ☀️</b></font> (with **KOR-gugugu-platypus-set** 🕊️)\n",
    "- <font color='#f2cc3b'><b>Model Upload 🤗</b></font>\n",
    "\n",
    "  \n",
    "### 3. 실습에 이용되는 모델과 데이터셋 📖\n",
    "\n",
    "- ✅ 모델: [yanolja/KoSOLAR-10.7B-v0.2](https://huggingface.co/yanolja/KoSOLAR-10.7B-v0.2)\n",
    "- ✅ 데이터셋: [kyujinpy/KOR-gugugu-platypus-set](https://huggingface.co/datasets/kyujinpy/KOR-gugugu-platypus-set)\n",
    "\n",
    "\n",
    "### 4. 참고자료\n",
    "\n",
    "- Model\n",
    "  - [upstage/SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)\n",
    "- Dataset\n",
    "  - [Platypus](https://github.com/arielnlee/Platypus)  \n",
    "  - [garage-bAInd/Open-Platypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)\n",
    "  - [squarelike/OpenOrca-gugugo-ko](https://huggingface.co/datasets/squarelike/OpenOrca-gugugo-ko)\n",
    "- Community\n",
    "  - [HuggingFace](https://huggingface.co/)  \n",
    "  - [KO-LLM LeaderBoard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)  \n",
    "  - [EN-LLM LeaderBoard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SXk0puNOpQ7"
   },
   "source": [
    "--------\n",
    "# ♾️ 환경설정\n",
    "\n",
    "- 🤗 transformers: LLM fine-tuning 및 전반적인 기능\n",
    "- 🤗 bitsandbytes: bit 단위로 모델을 불러올 수 있는 기능\n",
    "- 🤗 accelerate: GPU 가속\n",
    "- 🔥 loralib: LoRA 세팅\n",
    "- 🤗 datasets: 데이터셋 활용\n",
    "- 💎 peft: Parameter Efficient Fine Tuning 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1706172928270,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "z72x_0sBUkNo"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1706172928983,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "gxwVD1IVVmez",
    "outputId": "dc89d4d5-95ec-42f8-b7ce-b61e11706ef7"
   },
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "# 👨‍💻 HuggingFace 로그인\n",
    "\n",
    "1. [HuggingFace 🤗](https://huggingface.co/)에서 자신의 Access token 준비한다.\n",
    "  - `Role: read` token과 `Role: write` token이 각각 필요\n",
    "\n",
    "2. 코드에서 HuggingFace login을 한다.\n",
    "  - ⭐`Role: read`로 생성된 token을 복사 한 후, 아래의 `<read_token> 부분에 붙여넣는다.\n",
    "```bash\n",
    "!huggingface-cli login --token <read_token>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface login\n",
    "\n",
    "# READ token\n",
    "!huggingface-cli login --token <read_token> # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtHstE0tWJrm"
   },
   "source": [
    "--------\n",
    "# 😎 모듈 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6833,
     "status": "ok",
     "timestamp": 1706172935812,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "f6-72g-XWINm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import fire\n",
    "import json\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict\n",
    ")\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upTfYsRNg7Dj"
   },
   "source": [
    "--------\n",
    "# 🤗 Base Model (LLM)\n",
    "- 이용할 base LLM 모델: [yanolja/KoSOLAR-10.7B-v0.2](https://huggingface.co/yanolja/KoSOLAR-10.7B-v0.2)     \n",
    "\n",
    "## ✔️ Model Access 확인\n",
    "> ✅ 모델을 이용하기 전에 access가 있는지 꼭 확인해야 한다!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1706172935812,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "nfiPHh5Sh0kp"
   },
   "outputs": [],
   "source": [
    "#@title 🤗 Base model 선택하기\n",
    "device = 'auto' #@param {type: \"string\"}\n",
    "base_LLM_model = 'yanolja/KoSOLAR-10.7B-v0.2' #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "09d1aa49266c4848b07376ce6470db15",
      "9fd773b4056f4ab892153de7c208a99f",
      "f90388bd7b344610b7c782926594ba46",
      "46fbcefcc729458495440e0a700e42db",
      "8090443a7efd48e6803a5def0c80bc28",
      "b015b6104287406887bc6c5f86053e8b",
      "1f482215a44d4a619f274ad5cb152e06",
      "b06c652739324012abc08c25f62e4ae1",
      "7d900010091e40c89811ee8416dc6df0",
      "0a35767733734f7ab1c882d4d395b301",
      "5c0d43473b714acaa2532ac8be0bc68e"
     ]
    },
    "executionInfo": {
     "elapsed": 15737,
     "status": "ok",
     "timestamp": 1706172951543,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "pIjvCtRJghoP",
    "outputId": "38f7d6d0-a29b-4dcf-b388-6c70e7f03fbf"
   },
   "outputs": [],
   "source": [
    "# 모델 다운로드 (~30분)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_LLM_model,\n",
    "    load_in_8bit=True, # LoRA\n",
    "    #load_in_4bit=True, # Quantization Load\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_LLM_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1706172951543,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "o1_K5QbnzxUP",
    "outputId": "2eccd66f-3b3a-4b15-eae9-92437b3074fc"
   },
   "outputs": [],
   "source": [
    "# BOS, EOS, PAD 토큰 확인\n",
    "\n",
    "# Check special token\n",
    "bos = tokenizer.bos_token_id # 문장 시작 토큰\n",
    "eos = tokenizer.eos_token_id # 문장 끝 토큰\n",
    "pad = tokenizer.pad_token_id # 문장 패딩 토큰\n",
    "tokenizer.padding_side = \"right\" # 패딩 오른쪽\n",
    "\n",
    "print(\"BOS token:\", bos) # 1\n",
    "print(\"EOS token:\", eos) # 32000\n",
    "print(\"PAD token:\", pad) # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1706172951544,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "WOBun1kv0m_8",
    "outputId": "1231a71a-23d8-40ad-a35f-0e9986897ab9"
   },
   "outputs": [],
   "source": [
    "if (pad == None) or (pad == eos):\n",
    "    tokenizer.pad_token_id = 0  # 만약 패딩값이 없거나 eos값과 같다면,\n",
    "print(\"length of tokenizer:\",len(tokenizer)) # 40960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1706172951544,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "URjFNnmc6f0e",
    "outputId": "cecee860-8cc6-4c51-ad59-bc3967c6dca4"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model)) # 모델의 타입 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmZIeJR3HCwN"
   },
   "source": [
    "--------\n",
    "# 💡 Hyperparameters setting\n",
    "\n",
    "1. **cutoff_len**\n",
    "> 모델에 들어갈 **sequence의 최대 길이를 설정**\n",
    "\n",
    "2. **warmup_steps**\n",
    "> warmup_steps은 천천히 learning rate를 올려서, 학습의 불안정성을 방지해주는 역할을 함\n",
    "  \n",
    "  - **Example)** learning rate (LR)을 1e-2로 설정했다면, LR을 처음에 초기값보다 낮게 시작하여 100 step에 걸쳐서 LR를 1e-2로 올린다.\n",
    "  \n",
    "3. **Optimizer**\n",
    "> 최근에는 Adam보다 AdamW를 더 많이 이용하는 추세임\n",
    "> AdamW는 Adam에 비해서 더 <font color='#ea7a7a'><b>general</b></font>하게 성능을 만들어주는 장점이 있음\n",
    "  \n",
    "  - Adam vs <font color='#ea7a7a'><b>**AdamW**</b></font>\n",
    "\n",
    "4. **lr_scheduler**\n",
    "> lr_scheduler는 learning rate를 훈련하는 동안 조절해주는 옵션\n",
    "> 아래의 목록에 있는 총 3가지 함수 옵션이 가장 대표적으로 사용됨\n",
    "  \n",
    "  - constant\n",
    "  - linear\n",
    "  - <font color='#ea7a7a'><b>consine</b></font>\n",
    "\n",
    "5. **Others**\n",
    "\n",
    "  - **⭐ Learning rate**\n",
    "  - Batch size\n",
    "  - weight_decay\n",
    "  - max grad norm: gradient vector 크기 조절 (gradient clipping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxrjlM6AdhJq"
   },
   "source": [
    "## 👨‍💻 LoRA hyperparameters\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?id=1ZoFUOPAyfWx80qk9Oa6b8oY0h1bJRoE0\" width=512>\n",
    "\n",
    "  -  <font color='#e79531'><b>LoRA_r</b></font> : LoRA adapter의 차원을 결정하는 파라미터\n",
    "  - Lora_alpha : LoRA adapter의 scaling 값을 결정하는 파라미터\n",
    "  - Lora_dropout : LoRA adapter의 dropout 파라미터\n",
    "  -  <font color='#e79531'><b>**Lora_target_modules**</b></font> : LoRA adapter를 적용할 layers\n",
    "  ```\n",
    "Lora_target_modules 종류:\n",
    "[\"embed_tokens\", \"lm_head\", \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", ...] # in llama2\n",
    "  ```\n",
    "\n",
    "  <br>\n",
    "  \n",
    "### 🔎 LoRA가 적용되는 코드 예제\n",
    "  \n",
    "```python\n",
    "class Linear(nn.Linear, LoRALayer):\n",
    "  # LoRA implemented in a dense layer\n",
    "  def __init__(\n",
    "          self,\n",
    "          in_features: int,\n",
    "          out_features: int,\n",
    "          r: int = 0,\n",
    "          lora_alpha: int = 1,\n",
    "          lora_dropout: float = 0.,\n",
    "          fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n",
    "          merge_weights: bool = True,\n",
    "          **kwargs\n",
    "      ):\n",
    "          nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "          LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                              merge_weights=merge_weights)\n",
    "          self.fan_in_fan_out = fan_in_fan_out\n",
    "          # 실제 훈련할 파라미터\n",
    "          if r > 0:\n",
    "              self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
    "              self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
    "              self.scaling = self.lora_alpha / self.r\n",
    "              # Freezing the pre-trained weight matrix\n",
    "              self.weight.requires_grad = False\n",
    "          self.reset_parameters()\n",
    "          if fan_in_fan_out:\n",
    "            self.weight.data = self.weight.data.transpose(0, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1706172951544,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "SphoNRsqHDNS"
   },
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터\n",
    "\n",
    "# 데이터셋과 훈련 횟수와 관련된 하이퍼 파라미터\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "micro_batch = 1\n",
    "gradient_accumulation_steps = batch_size // micro_batch\n",
    "\n",
    "# 훈련 방법에 대한 하이퍼 파라미터\n",
    "cutoff_len = 4096\n",
    "lr_scheduler = 'cosine'\n",
    "warmup_ratio = 0.06 # warmup_steps = 100\n",
    "learning_rate = 4e-4\n",
    "optimizer = 'adamw_torch'\n",
    "weight_decay = 0.01\n",
    "max_grad_norm = 1.0 # default: 1.0\n",
    "neftune_noise_alpha = 15\n",
    "\n",
    "# LoRA config\n",
    "lora_r = 8\n",
    "lora_alpha = 8\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "\n",
    "# Tokenizer에서 나오는 input값 설정 옵션\n",
    "train_on_inputs = False\n",
    "add_eos_token = False\n",
    "\n",
    "# Others\n",
    "output_dir = './tuned_LLM'\n",
    "\n",
    "resume_from_checkpoint = False # !! 만약 모델을 이어서 훈련하고 싶다면, 아래 주석처럼 './custom_LLM/checkpoint-[xxx]'와 같이 파일 경로를 입력해야 함\n",
    "# resume_from_checkpoint = f\"{output_dir}/checkpoint-100\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNDw8EcnDgy0"
   },
   "source": [
    "--------\n",
    "# 🤗 Dataset Loading and formatting\n",
    "> 👨‍💻 LLM fine-tuning을 위한 데이터셋을 불러오고 <font color='green'><b>instruction input-output</b></font>형태로 만들기\n",
    "\n",
    "- 이용할 데이터셋: [kyujinpy/KOR-gugugu-platypus-set](https://huggingface.co/datasets/kyujinpy/KOR-gugugu-platypus-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1706172951544,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "uc6eJ15pD3oq"
   },
   "outputs": [],
   "source": [
    "#@title 🤗 Choose Dataset\n",
    "dataset = 'kyujinpy/KOR-gugugu-platypus-set' #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1154,
     "status": "ok",
     "timestamp": 1706172952678,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "t7Ywoq65DevA",
    "outputId": "992aab05-fa21-4704-9519-75e5bc7b1a9c"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 다운로드\n",
    "\n",
    "data = load_dataset(dataset)\n",
    "print(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrsYtTRuof_W"
   },
   "source": [
    "## 🤗 Instruction input-output template\n",
    "> 데이터는 아래와 같은 형식을 갖고 있음\n",
    "   \n",
    "### ✅ Input이 있는 example:  \n",
    "  \n",
    "```\n",
    "Input:\n",
    "당신은 사람들이 정보를 찾을 수 있도록 도와주는 인공지능 비서입니다.\n",
    "사용자가 질문을 합니다. 당신의 임무는 가능한 한 충실하게 대답하는 것입니다.\n",
    "답변하는 동안 단계별로 생각하고 답변을 정당화하세요.  \n",
    "  \n",
    "Instruction:\n",
    "질문입니다: 시속 60km의 속도로 달리는 기차가 9초 만에 기둥을 통과합니다. 기차의 길이는 얼마입니까?\n",
    "옵션: (A) 287 (B) 699 (C) 677 (D) 168 (E) 150 먼저 생각해 봅시다. 무작위로 추론해 봅시다:\n",
    "  \n",
    "Output:\n",
    "기차의 길이를 구하려면 기차가 기둥을 통과하는 동안 이동하는 거리를 결정해야 합니다.\n",
    "1단계: 기차의 속도를 초당 미터로 변환합니다. 1km = 1000미터, 1시간 = 3600초, 따라서 60km/hr = (60 * 1000) / 3600 = 16.67m/s(대략).\n",
    "2단계: 기차가 9초 동안 이동한 거리를 계산합니다. 거리 = 속도 × 시간 거리 = 16.67m/s * 9초 = 150미터.\n",
    "3단계: 계산된 거리를 주어진 옵션과 비교합니다.\n",
    "계산된 거리(150미터)에 가장 가까운 옵션은 옵션 (E) 150입니다.\n",
    "따라서 열차의 길이는 약 150미터입니다.\n",
    "```\n",
    "\n",
    "  \n",
    "### ✅ Input이 없는 example:\n",
    "\n",
    "```\n",
    "Instruction:\n",
    "질문이 주어집니다: 다음 기사에 대한 객관식 문제를 작성하세요: 기사: 겨울은 무슨 일이 일어날지 알기 어렵고 사고가 너무 쉽게 일어나기 때문에 위험합니다.\n",
    "안개가 언덕 꼭대기에서 여러분을 기다리고 있을 수 있습니다. 녹는 눈 밑에 얼음이 숨어 있다가 운전자를 도로 밖으로 내보내려고 기다리고 있을 수도 있습니다.\n",
    "마주 오는 차가 갑자기 도로를 가로질러 미끄러질 수도 있습니다. 빙판길 운전의 첫 번째 규칙은 부드럽게 운전하는 것입니다.\n",
    "갑작스러운 움직임은 자동차를 제어하기 매우 어렵게 만들 수 있습니다. 따라서 차를 출발하거나 정지할 때마다, 속도를 높이거나 낮출 때마다 가능한 한 부드럽고 천천히 운전해야 합니다.\n",
    "옆 좌석에 뜨거운 커피 한 잔을 가득 들고 운전한다고 가정해 보겠습니다. 커피를 엎지르지 않도록 운전하세요.\n",
    "두 번째 규칙은 무슨 일이 일어날지 주의를 기울이는 것입니다. 얼음이 많을수록 도로 아래쪽을 더 잘 살펴야 합니다.\n",
    "차를 부드럽게 정지하는 데 걸리는 시간을 테스트하세요. 생각보다 더 빨리 운전할 수 있다는 점을 기억하세요.\n",
    "일반적으로 도로가 젖어 있을 때는 평소 정지 거리의 두 배, 눈이 쌓여 있을 때는 이 거리의 세 배, 빙판길에서는 그보다 더 많은 거리를 확보하세요.\n",
    "항상 차량을 통제하려고 노력하면 문제가 발생하지 않습니다. 정답은\n",
    "\n",
    "Output:\n",
    "빙판길 운전 시 가장 중요한 규칙은 무엇인가요?\n",
    "A) 위험 지역을 더 빨리 통과하기 위해 속도를 높입니다.\n",
    "B) 잠재적 위험을 피하기 위한 갑작스러운 움직임\n",
    "C) 부드럽고 부드럽게 운전하세요\n",
    "D) 바로 앞 도로에만 집중하기\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idkBb8qpXjQN"
   },
   "source": [
    "## 🔎 Alpaca template을 활용하여 데이터셋을 instruction input-output 형식으로 재정의\n",
    "- [Alpaca github](https://github.com/tatsu-lab/stanford_alpaca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJHw60DcXkmO"
   },
   "source": [
    "### ✅ 한국어 버전 template\n",
    "```\n",
    "{\n",
    "    \"prompt_input\": \"아래는 작업을 설명하는 지침과 추가 입력을 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 답변을 작성해주세요.\\n\\n### 지침:\\n{instruction}\\n\\n### 입력:\\n{input}\\n\\n### 답변:\\n\",\n",
    "    \"prompt_no_input\" : \"아래는 작업을 설명하는 지침입니다. 요청을 적절히 완료하는 답변을 작성해주세요.\\n\\n### 지침:\\n{instruction}\\n\\n### 답변:\\n\",\n",
    "    \"response_split\": \"### 답변:\"\n",
    "}\n",
    "```\n",
    "\n",
    "### ✅ (참고) 영어 버전 template\n",
    "```\n",
    "{\n",
    "    \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "    \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
    "    \"response_split\": \"### Response:\"    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1706172952679,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "NK6EyQexOTBj"
   },
   "outputs": [],
   "source": [
    "# Instruction tuning을 위한 한국어 template 작성.\n",
    "\n",
    "instruct_template = {\n",
    "    \"prompt_input\": \"아래는 작업을 설명하는 지침과 추가 입력을 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 답변을 작성해주세요.\\n\\n### 지침:\\n{instruction}\\n\\n### 입력:\\n{input}\\n\\n### 답변:\\n\",\n",
    "    \"prompt_no_input\" : \"아래는 작업을 설명하는 지침입니다. 요청을 적절히 완료하는 답변을 작성해주세요.\\n\\n### 지침:\\n{instruction}\\n\\n### 답변:\\n\",\n",
    "    \"response_split\": \"### 답변:\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1706172952679,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "Z8f63aofNTWP"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 불러오는 클래스\n",
    "\n",
    "class Prompter(object):\n",
    "\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        self.template = instruct_template\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "\n",
    "        if input: # input text가 있다면\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "prompter = Prompter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1706172952679,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "dWdjgMS6Gi6x"
   },
   "outputs": [],
   "source": [
    "# Token generation 함수\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"input\"],\n",
    "        data_point[\"output\"])\n",
    "\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"], data_point[\"input\"])\n",
    "\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token)\n",
    "\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "be359c2e9b23413686abd38db766738f",
      "b40a7e5d99af4b4ba6ee21cdbadb88f6",
      "17bde28dfca84b8cab6fa2f82f0b22ff",
      "533390dd23894cdd81e4d12ea8699e25",
      "2aab39d937724f6ab13388ee5e34a325",
      "4cf2875b85ff42068e909d88f20efd2a",
      "17bf32bff22040719d14afe10ae37ac3",
      "ad64d173130d46d88d19ad9c99369ca7",
      "cbb0f91b980a4f82847fd299d33585ec",
      "cad710ea25074813bc9157e493be83c3",
      "b3aef855c13b45cfab75fed98484eae8"
     ]
    },
    "executionInfo": {
     "elapsed": 86498,
     "status": "ok",
     "timestamp": 1706173039168,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "p-ncCWzWQr4h",
    "outputId": "e9ba59f0-021b-458e-efae-6518b200e0a3"
   },
   "outputs": [],
   "source": [
    "# 훈련 셋 만들기 (~2분)\n",
    "\n",
    "val_data = None\n",
    "train_data = data[\"train\"].shuffle() # random\n",
    "train_data = train_data.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8RcQ-wiSr1N"
   },
   "source": [
    "### ✅ 'prompt_input' example:\n",
    "\n",
    "```\n",
    "아래는 작업을 설명하는 지침과 추가 입력을 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 답변을 작성해주세요.\n",
    "\n",
    "### 지침:\n",
    "다음 질문에 답하세요: 제목: 내가 본 최악의 영화 리뷰: 이것은 내가 본 최악의 영화입니다... DVD를 구입하는 유일한 이유는 이 영화를 만든 12살짜리 꼬마가 저능아 제작자에게 팔아넘긴 진짜 감독의 코멘터리를 공개하기 위해서입니다. 가장 좋은 부분은 비디오 게임 클립을 이어 붙인 것을 볼 때였을 것입니다. 이 상품평은 부정적인가요?\n",
    "답변:\n",
    "\n",
    "### 입력:\n",
    "귀하는 지시를 매우 잘 따르는 인공지능 비서입니다. 최대한 많이 도와주세요.\n",
    "\n",
    "### 답변:\n",
    "예, 이 제품 리뷰는 부정적입니다.</s>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NCt1X--S1Hv"
   },
   "source": [
    "### ✅ 'prompt_no_input' example:\n",
    "\n",
    "```\n",
    "아래는 작업을 설명하는 지침입니다. 요청을 적절히 완료하는 답변을 작성해주세요.\n",
    "\n",
    "### 지침:\n",
    "x+y = 10$, $2x+y = 13$이 주어졌을 때, $x^2-y^2$를 평가합니다.\n",
    "\n",
    "### 답변:\n",
    "x^2-y^2$를 풀려면 $x$와 $y$의 값을 찾아야 합니다. 주어진 방정식을 사용하여 변수 중 하나를 제거하고 다른 변수를 풀 수 있습니다. 첫 번째 방정식을 두 번째 방정식에서 빼면 $2x+y - (x+y) = 13-10$이 나오며, 이는 $x = 3$으로 단순화됩니다. 그런 다음 첫 번째 방정식을 사용하여 $x = 3$을 대입하면 $3+y = 10$을 얻을 수 있으며, 이는 $y = 7$이라는 것을 의미합니다. 이제 $x = 3$과 $y = 7$을 얻었으므로 이를 $x^2-y^2$로 대입할 수 있습니다. 이렇게 하면 $3^2-7^2 = 9-49 = -40$이 됩니다.</s>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LByHr8qpRk_8"
   },
   "source": [
    "--------\n",
    "# 🤗 Apply LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1706173039168,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "6M37ePijRGws"
   },
   "outputs": [],
   "source": [
    "# LoRA config 정의\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1706173039639,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "CMB2WzT0SfGM",
    "outputId": "1b2eef6d-1aa3-49b7-846f-239102c846d0"
   },
   "outputs": [],
   "source": [
    "# Model with LoRA\n",
    "\n",
    "model = prepare_model_for_int8_training(model)\n",
    "model = get_peft_model(model, config) # Applying LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFEsYeMYTsUf"
   },
   "source": [
    "--------\n",
    "# ⭐ LLM fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1706173039639,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "4tmSdPXhStNA",
    "outputId": "c50411e2-0206-44d6-e919-5c75c793776e"
   },
   "outputs": [],
   "source": [
    "# 만약 이전에 돌렸던 모델을 가져온다면, 아래의 코드 실행\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # All checkpoint\n",
    "\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model\n",
    "        resume_from_checkpoint = (\n",
    "            True\n",
    "        ) # kyujin: I will use this checkpoint\n",
    "\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1706173040326,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "DtZpAMd1TlMV",
    "outputId": "1894cd92-a0c3-4046-d843-40cd5c889e75"
   },
   "outputs": [],
   "source": [
    "# Trainer class 정의\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments( \n",
    "            # 훈련에 이용될 하이퍼파라미터\n",
    "            per_device_train_batch_size = micro_batch,\n",
    "            gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            optim=optimizer,\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            neftune_noise_alpha=neftune_noise_alpha,\n",
    "            max_grad_norm = max_grad_norm,\n",
    "            save_steps = 13, # you can change!\n",
    "            lr_scheduler_type=lr_scheduler,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=False,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            group_by_length = False\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.print_trainable_parameters() # 훈련하는 파라미터의 % 체크\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2207906,
     "status": "error",
     "timestamp": 1706175248228,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "DX8RW5YPUxWY",
    "outputId": "f317e0bf-ddee-4690-cf72-03d6127c94d8"
   },
   "outputs": [],
   "source": [
    "# Training (fine-tuning)\n",
    "# 훈련시간이 많이 소요됨\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 700,
     "status": "ok",
     "timestamp": 1706175260927,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "Ad0OAYkR0TtG",
    "outputId": "35fedf47-7662-4016-cfa4-a848fb294c7c"
   },
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "\n",
    "print(f\"output_dir: {output_dir}\")\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "model_path = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "torch.save({}, model_path)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NNao1MNzUtB"
   },
   "source": [
    "-------\n",
    "# 🤗 LoRA Adapter merge\n",
    "> LLM fine-tuning 후, 훈련한 LoRA layer를 기존 base LLM에 붙여서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1706175248229,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "LRcka4btzTMO"
   },
   "outputs": [],
   "source": [
    "# 훈련된 LoRA layer와 base LLM 병합 (merge)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_LLM_model,\n",
    "    return_dict = True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, output_dir, device)\n",
    "model = model.merge_and_unload() # Merge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1706175248229,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "Qe6w9jvnzTFz"
   },
   "outputs": [],
   "source": [
    "# custom LLM 모델 저장!\n",
    "\n",
    "final_save_folder = './custom_model'\n",
    "\n",
    "model.save_pretrained(final_save_folder)\n",
    "tokenizer.save_pretrained(final_save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8ViOCSH_8Ha"
   },
   "source": [
    "-------\n",
    "# 🤗 HuggingFace에 모델 업로드\n",
    "\n",
    "1. 코드에서 Huggingface login 진행\n",
    "    - HuggingFace에 다시 접속하여, access token 카테고리 접속한다.\n",
    "    - ⭐`Role: write`로 생성된 token을 복사 한 후, 아래의 `<write_token>` 부분에 붙여넣는다.\n",
    " \n",
    "        ```bash\n",
    "        !huggingface-cli login --token <write_token>\n",
    "        ```\n",
    "        <br>\n",
    "\n",
    "2. ✅ Huggingface에 접속하여, 모델을 업로드 할 repo를 생성한다.\n",
    "3. ✅ 아래의 코드를 이용하여 모델과 tokenizer를 업로드한다.\n",
    "    ```python\n",
    "    model = AutoModelForCausalLM.from_pretrained(final_save_folder)\n",
    "    model.push_to_hub(\"<repo_name>\", token=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(final_save_folder)\n",
    "    tokenizer.push_to_hub(\"<repo_name>\", token=True)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1706175248229,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "TtWH4V9nVfYl"
   },
   "outputs": [],
   "source": [
    "# WRITE token\n",
    "!huggingface-cli login --token <write_token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1706175248229,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "S76GCggGzMD5"
   },
   "outputs": [],
   "source": [
    "# 모델 업로드 (~10분)\n",
    "## 먼저, 허깅페이스에 모델 repo를 만든 후 코드를 실행해야 한다!\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(final_save_folder)\n",
    "model.push_to_hub(\"oneonlee/KoSOLAR-v0.2-gugutypus-10.7B\", token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1706175248229,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "nT3MeKo02vCd"
   },
   "outputs": [],
   "source": [
    "# Tokenizer 업로드\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_save_folder)\n",
    "tokenizer.push_to_hub(\"oneonlee/KoSOLAR-v0.2-gugutypus-10.7B\", token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1706175248229,
     "user": {
      "displayName": "이동건",
      "userId": "00608305585497627615"
     },
     "user_tz": -540
    },
    "id": "3En7kM-dBhES"
   },
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1tltUXnPZyd_BKIfJVEgpAMqHYMWCk_-B",
     "timestamp": 1705843685078
    },
    {
     "file_id": "1ajjaAkbzRna7Udjf0WAH406jMULnYIY1",
     "timestamp": 1700589110156
    }
   ]
  },
  "kernelspec": {
   "display_name": "venv_kernel",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09d1aa49266c4848b07376ce6470db15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9fd773b4056f4ab892153de7c208a99f",
       "IPY_MODEL_f90388bd7b344610b7c782926594ba46",
       "IPY_MODEL_46fbcefcc729458495440e0a700e42db"
      ],
      "layout": "IPY_MODEL_8090443a7efd48e6803a5def0c80bc28"
     }
    },
    "0a35767733734f7ab1c882d4d395b301": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17bde28dfca84b8cab6fa2f82f0b22ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad64d173130d46d88d19ad9c99369ca7",
      "max": 51170,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cbb0f91b980a4f82847fd299d33585ec",
      "value": 51170
     }
    },
    "17bf32bff22040719d14afe10ae37ac3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f482215a44d4a619f274ad5cb152e06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2aab39d937724f6ab13388ee5e34a325": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46fbcefcc729458495440e0a700e42db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a35767733734f7ab1c882d4d395b301",
      "placeholder": "​",
      "style": "IPY_MODEL_5c0d43473b714acaa2532ac8be0bc68e",
      "value": " 5/5 [00:12&lt;00:00,  2.19s/it]"
     }
    },
    "4cf2875b85ff42068e909d88f20efd2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "533390dd23894cdd81e4d12ea8699e25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cad710ea25074813bc9157e493be83c3",
      "placeholder": "​",
      "style": "IPY_MODEL_b3aef855c13b45cfab75fed98484eae8",
      "value": " 51170/51170 [01:26&lt;00:00, 522.17 examples/s]"
     }
    },
    "5c0d43473b714acaa2532ac8be0bc68e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d900010091e40c89811ee8416dc6df0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8090443a7efd48e6803a5def0c80bc28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fd773b4056f4ab892153de7c208a99f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b015b6104287406887bc6c5f86053e8b",
      "placeholder": "​",
      "style": "IPY_MODEL_1f482215a44d4a619f274ad5cb152e06",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "ad64d173130d46d88d19ad9c99369ca7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b015b6104287406887bc6c5f86053e8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b06c652739324012abc08c25f62e4ae1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3aef855c13b45cfab75fed98484eae8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b40a7e5d99af4b4ba6ee21cdbadb88f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cf2875b85ff42068e909d88f20efd2a",
      "placeholder": "​",
      "style": "IPY_MODEL_17bf32bff22040719d14afe10ae37ac3",
      "value": "Map: 100%"
     }
    },
    "be359c2e9b23413686abd38db766738f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b40a7e5d99af4b4ba6ee21cdbadb88f6",
       "IPY_MODEL_17bde28dfca84b8cab6fa2f82f0b22ff",
       "IPY_MODEL_533390dd23894cdd81e4d12ea8699e25"
      ],
      "layout": "IPY_MODEL_2aab39d937724f6ab13388ee5e34a325"
     }
    },
    "cad710ea25074813bc9157e493be83c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbb0f91b980a4f82847fd299d33585ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f90388bd7b344610b7c782926594ba46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b06c652739324012abc08c25f62e4ae1",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7d900010091e40c89811ee8416dc6df0",
      "value": 5
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
